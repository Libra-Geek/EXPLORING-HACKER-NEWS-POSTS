{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **INTRODUCTION**\n",
    "\n",
    "\n",
    "- The learning objectives of this lesson have been added in the README file attached to this repository. However, to understand this project clearly, you should check out my lesson on [how to work with complex dates and times in Python](https://github.com/Tess-hacker/WORKING-WITH-COMPLEX-DATE-TIME-DATASET-IN-PYTHON). You'll find it interesting, I promise!\n",
    "\n",
    "\n",
    "### **A Little Background on Hacker News**\n",
    "\n",
    "- Hacker News is a site started by the startup incubator Y Combinator, where user-submitted stories (known as \"posts\") are voted and commented upon, similar to reddit. Hacker News is extremely popular in technology and startup circles, and posts that make it to the top of Hacker News' listings can get hundreds of thousands of visitors as a result.\n",
    "\n",
    "\n",
    "- As stated in the README, we would be making use of the dataset [here](https://www.kaggle.com/hacker-news/hacker-news-posts/home). Below are the description of the columns: \n",
    "\n",
    "    - `id`: The unique identifier from Hacker News for the post\n",
    "    \n",
    "    - `title`: The title of the post\n",
    "    \n",
    "    - `url`: The URL that the posts links to, if it the post has a URL\n",
    "    \n",
    "    - `num_points`: The number of points the post acquired, calculated as the total number of upvotes minus the total number of downvotes\n",
    "    \n",
    "    - `num_comments`: The number of comments that were made on the post\n",
    "    \n",
    "    - `author`: The username of the person who submitted the post\n",
    "    \n",
    "    - `created_at`: The date and time at which the post was submitted\n",
    "    \n",
    "    \n",
    "- In this project, we are specifically interested in the posts whose titles begin with `Ask HN` or `Show HN`. The former are posts used to ask the Hacker News Community a specific question while the latter are posts used to show the community a project, product or something generally interesting. We will be **comparing these two posts to answer the following questions:**\n",
    "\n",
    "    - Do `Ask HN` or `Show HN` receive more comments on average?\n",
    "    \n",
    "    - Do posts created at a certain time receive more comments on average?\n",
    "\n",
    "\n",
    "- Let us begin by importing the required libraries we need to execute these tasks and reading the dataset into the list of lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['12579008', 'You have two days to comment if you want stem cells to be classified as your own', 'http://www.regulations.gov/document?D=FDA-2015-D-3719-0018', '1', '0', 'altstar', '9/26/2016 3:26'], ['12579005', 'SQLAR  the SQLite Archiver', 'https://www.sqlite.org/sqlar/doc/trunk/README.md', '1', '0', 'blacksqr', '9/26/2016 3:24'], ['12578997', 'What if we just printed a flatscreen television on the side of our boxes?', 'https://medium.com/vanmoof/our-secrets-out-f21c1f03fdc8#.ietxmez43', '1', '0', 'pavel_lishin', '9/26/2016 3:19'], ['12578989', 'algorithmic music', 'http://cacm.acm.org/magazines/2011/7/109891-algorithmic-composition/fulltext', '1', '0', 'poindontcare', '9/26/2016 3:16'], ['12578979', 'How the Data Vault Enables the Next-Gen Data Warehouse and Data Lake', 'https://www.talend.com/blog/2016/05/12/talend-and-Â\\x93the-data-vaultÂ\\x94', '1', '0', 'markgainor1', '9/26/2016 3:14'], ['12578975', 'Saving the Hassle of Shopping', 'https://blog.menswr.com/2016/09/07/whats-new-with-your-style-feed/', '1', '1', 'bdoux', '9/26/2016 3:13'], ['12578954', \"Macalifa  A new open-source music app for UWP that won't suck\", 'http://forums.windowscentral.com/windows-phone-apps/440523-macalifa-new-open-source-music-app-uwp-wont-suck.html', '1', '0', 'thecodrr', '9/26/2016 3:06'], ['12578942', 'GitHub  theweavrs/Macalifa: A music player written for UWP', 'https://github.com/theweavrs/Macalifa', '1', '0', 'thecodrr', '9/26/2016 3:04'], ['12578919', 'Google Allo  first Impression', 'http://prodissues.com/2016/09/google-allo-first-impression.html', '3', '0', 'jandll', '9/26/2016 2:57'], ['12578918', 'Advanced Multimedia on the Linux Command Line', 'https://avi.alkalay.net/2016/09/multimedia-linux-cli.html', '1', '0', 'mynameislegion', '9/26/2016 2:56']]\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import unicodecsv as csv\n",
    "opened_file = open(r'C:\\Users\\USER\\Documents\\ONLINE COURSES\\DATAQUEST\\DATASETS\\HN_posts_year_to_Sep_26_2016.csv', 'r', encoding='utf-8')\n",
    "read_file = reader(opened_file)\n",
    "hn =list(read_file)\n",
    "hn= hn[1:] #we have eliminated the header\n",
    "print (hn[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let us assign the first row to a variable named *headers*. This will enable us to have our dataset rid of the headers and we can use both parties individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset headers are:\n",
      "[['12579008', 'You have two days to comment if you want stem cells to be classified as your own', 'http://www.regulations.gov/document?D=FDA-2015-D-3719-0018', '1', '0', 'altstar', '9/26/2016 3:26']]\n",
      "\n",
      "\n",
      "The newly extracted dataset are:\n",
      "[['12579005', 'SQLAR  the SQLite Archiver', 'https://www.sqlite.org/sqlar/doc/trunk/README.md', '1', '0', 'blacksqr', '9/26/2016 3:24'], ['12578997', 'What if we just printed a flatscreen television on the side of our boxes?', 'https://medium.com/vanmoof/our-secrets-out-f21c1f03fdc8#.ietxmez43', '1', '0', 'pavel_lishin', '9/26/2016 3:19'], ['12578989', 'algorithmic music', 'http://cacm.acm.org/magazines/2011/7/109891-algorithmic-composition/fulltext', '1', '0', 'poindontcare', '9/26/2016 3:16'], ['12578979', 'How the Data Vault Enables the Next-Gen Data Warehouse and Data Lake', 'https://www.talend.com/blog/2016/05/12/talend-and-Â\\x93the-data-vaultÂ\\x94', '1', '0', 'markgainor1', '9/26/2016 3:14'], ['12578975', 'Saving the Hassle of Shopping', 'https://blog.menswr.com/2016/09/07/whats-new-with-your-style-feed/', '1', '1', 'bdoux', '9/26/2016 3:13'], ['12578954', \"Macalifa  A new open-source music app for UWP that won't suck\", 'http://forums.windowscentral.com/windows-phone-apps/440523-macalifa-new-open-source-music-app-uwp-wont-suck.html', '1', '0', 'thecodrr', '9/26/2016 3:06']]\n"
     ]
    }
   ],
   "source": [
    "headers = hn[:1]\n",
    "print ('The dataset headers are:')\n",
    "print (headers)\n",
    "print ('\\n')\n",
    "hn = hn[1:]\n",
    "print ('The newly extracted dataset are:')\n",
    "print (hn[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that we have our dataset separated from the headers, we are ready to filter the data. Remember the titles of posts that are the area of focus for this project. Now, this is a large dataset and we would need a Python function to filter the dataset for the posts which we want.\n",
    "\n",
    "\n",
    "- The function `startswith` helps us to check through a dataset and return the variables that contain the assigned values which we have passed into the argument. In return, we get boolean values of True or False. Using this method, we would filter through our dataset for the posts that fall into the two categories which are the targets of this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of Ask Posts are:\n",
      "9139\n",
      "The total number of Show Posts are:\n",
      "10158\n",
      "The total number of Other Posts are:\n",
      "273821\n"
     ]
    }
   ],
   "source": [
    "ask_posts = []\n",
    "show_posts = []\n",
    "other_posts = []\n",
    "for row in hn:\n",
    "    title = row[1]\n",
    "    if title.lower().startswith(\"ask hn\"):\n",
    "        ask_posts.append(row)\n",
    "    elif title.lower().startswith(\"show hn\"):\n",
    "        show_posts.append(row)\n",
    "    else:\n",
    "        other_posts.append(row)\n",
    "print ('The total number of Ask Posts are:')\n",
    "print (len(ask_posts))\n",
    "print ('The total number of Show Posts are:')\n",
    "print (len(show_posts))\n",
    "print ('The total number of Other Posts are:')\n",
    "print (len(other_posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Five Rows of Ask Posts are:\n",
      "[['12578908', 'Ask HN: What TLD do you use for local development?', '', '4', '7', 'Sevrene', '9/26/2016 2:53'], ['12578522', 'Ask HN: How do you pass on your work when you die?', '', '6', '3', 'PascLeRasc', '9/26/2016 1:17'], ['12577908', 'Ask HN: How a DNS problem can be limited to a geographic region?', '', '1', '0', 'kuon', '9/25/2016 22:57'], ['12577870', 'Ask HN: Why join a fund when you can be an angel?', '', '1', '3', 'anthony_james', '9/25/2016 22:48'], ['12577647', 'Ask HN: Someone uses stock trading as passive income?', '', '5', '2', '00taffe', '9/25/2016 21:50']]\n",
      "\n",
      "\n",
      "The First Five Rows of Show Posts are:\n",
      "[['12578335', 'Show HN: Finding puns computationally', 'http://puns.samueltaylor.org/', '2', '0', 'saamm', '9/26/2016 0:36'], ['12578182', 'Show HN: A simple library for complicated animations', 'https://christinecha.github.io/choreographer-js/', '1', '0', 'christinecha', '9/26/2016 0:01'], ['12578098', 'Show HN: WebGL visualization of DNA sequences', 'http://grondilu.github.io/dna.html', '1', '0', 'grondilu', '9/25/2016 23:44'], ['12577991', 'Show HN: Pomodoro-centric, heirarchical project management with ES6 modules', 'https://github.com/jakebian/zeal', '2', '0', 'dbranes', '9/25/2016 23:17'], ['12577142', 'Show HN: Jumble  Essays on the go #PaulInYourPocket', 'https://itunes.apple.com/us/app/jumble-find-startup-essay/id1150939197?ls=1&mt=8', '1', '1', 'ryderj', '9/25/2016 20:06']]\n",
      "\n",
      "\n",
      "The First Five Rows of Other Posts are:\n",
      "[['12579005', 'SQLAR  the SQLite Archiver', 'https://www.sqlite.org/sqlar/doc/trunk/README.md', '1', '0', 'blacksqr', '9/26/2016 3:24'], ['12578997', 'What if we just printed a flatscreen television on the side of our boxes?', 'https://medium.com/vanmoof/our-secrets-out-f21c1f03fdc8#.ietxmez43', '1', '0', 'pavel_lishin', '9/26/2016 3:19'], ['12578989', 'algorithmic music', 'http://cacm.acm.org/magazines/2011/7/109891-algorithmic-composition/fulltext', '1', '0', 'poindontcare', '9/26/2016 3:16'], ['12578979', 'How the Data Vault Enables the Next-Gen Data Warehouse and Data Lake', 'https://www.talend.com/blog/2016/05/12/talend-and-Â\\x93the-data-vaultÂ\\x94', '1', '0', 'markgainor1', '9/26/2016 3:14'], ['12578975', 'Saving the Hassle of Shopping', 'https://blog.menswr.com/2016/09/07/whats-new-with-your-style-feed/', '1', '1', 'bdoux', '9/26/2016 3:13']]\n"
     ]
    }
   ],
   "source": [
    "print ('The First Five Rows of Ask Posts are:')\n",
    "print (ask_posts[:5])\n",
    "print ('\\n')\n",
    "print ('The First Five Rows of Show Posts are:')\n",
    "print (show_posts[:5])\n",
    "print ('\\n')\n",
    "print ('The First Five Rows of Other Posts are:')\n",
    "print (other_posts[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we would like to  determine whether the ask posts or the show posts receive more comments on the average.\n",
    "\n",
    "\n",
    "- The following are the steps that would be taken:\n",
    "\n",
    "    - **Determine the total number of comments for each category**\n",
    "    \n",
    "    - **Compute the average number of comments for each category**\n",
    "    \n",
    "\n",
    "- Afterwards, we would conclude on whether the show or ask posts receive the highest number of average comments.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average comments for the ask posts are:\n",
      "10.393478498741656\n",
      "The average comments for the show posts are:\n",
      "4.886099625910612\n"
     ]
    }
   ],
   "source": [
    "total_ask_comments = 0\n",
    "for row in ask_posts:\n",
    "    total_ask_comments += int(row[4])\n",
    "    average_ask_comments = total_ask_comments/len(ask_posts)\n",
    "print ('The average comments for the ask posts are:')\n",
    "print (average_ask_comments)\n",
    "\n",
    "total_show_comments = 0\n",
    "for comments in show_posts:\n",
    "    total_show_comments += int(comments[4])\n",
    "    average_show_comments = total_show_comments/len(show_posts)\n",
    "print ('The average comments for the show posts are:')\n",
    "print (average_show_comments)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CALCULATING THE NUMBER OF ASK POSTS AND COMMENTS BASED ON THE HOUR CREATED**\n",
    "\n",
    "\n",
    "- Here, we would like to fully determine whether the ask posts and comments vary by the timeframe within which they are created. Since we have ascertained from the code above that the ask posts has the highest average comment, then, the time period needs to be taken into consideration too. \n",
    "\n",
    "\n",
    "- To calculate this successfully, we would need to follow the following steps:\n",
    "\n",
    "    - First, we would calculate the amount of ask posts created per hour of the day along with the comments received.\n",
    "    \n",
    "    - Then, we would calculate the average number of comments that the ask posts receive per hour created. \n",
    "    \n",
    "    \n",
    "- Of course, you should know that the `datetime` module would come in handy now in order to deal with and execute these tasks. Recall that we ca use the `datetime.strptimr` to parse the dates stored as strings and to return `datetime` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'02': 2996, '01': 2089, '22': 3372, '21': 4500, '19': 3954, '17': 5547, '15': 18525, '14': 4972, '13': 7245, '11': 2797, '10': 3013, '09': 1477, '07': 1585, '03': 2154, '23': 2297, '20': 4462, '16': 4466, '08': 2362, '00': 2277, '18': 4877, '12': 4234, '04': 2360, '06': 1587, '05': 1838}\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "result_list = []\n",
    "for row in ask_posts:\n",
    "    created_at = row[6]\n",
    "    comments = row[4]\n",
    "    result_list.append([created_at, int(comments)])\n",
    "    \n",
    "counts_by_hour = {}\n",
    "comments_by_hour = {}\n",
    "ask_date_format = \"%m/%d/%Y %H:%M\"\n",
    "for row in result_list:\n",
    "    date = row[0]\n",
    "    comment = row[1]\n",
    "    askpost_time = dt.datetime.strptime(date, ask_date_format).strftime(\"%H\")\n",
    "    if askpost_time in counts_by_hour:\n",
    "        comments_by_hour[askpost_time] += comment\n",
    "        counts_by_hour[askpost_time] += 1\n",
    "    else:\n",
    "        comments_by_hour[askpost_time] =comment\n",
    "        counts_by_hour[askpost_time] = 1\n",
    "print (comments_by_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['02', 11.137546468401487], ['01', 7.407801418439717], ['22', 8.804177545691905], ['21', 8.687258687258687], ['19', 7.163043478260869], ['17', 9.449744463373083], ['15', 28.676470588235293], ['14', 9.692007797270955], ['13', 16.31756756756757], ['11', 8.96474358974359], ['10', 10.684397163120567], ['09', 6.653153153153153], ['07', 7.013274336283186], ['03', 7.948339483394834], ['23', 6.696793002915452], ['20', 8.749019607843136], ['16', 7.713298791018998], ['08', 9.190661478599221], ['00', 7.5647840531561465], ['18', 7.94299674267101], ['12', 12.380116959064328], ['04', 9.7119341563786], ['06', 6.782051282051282], ['05', 8.794258373205741]]\n"
     ]
    }
   ],
   "source": [
    "# Now, we would execute the second step of our task by calculating the average number of comments that the ask posts receive per hour created.\n",
    "# The two dictionaries created earlier would be used to perform this task.\n",
    "avg_by_hour = []\n",
    "for hour in comments_by_hour:\n",
    "    avg_by_hour.append([hour, comments_by_hour[hour] / counts_by_hour[hour]])\n",
    "print (avg_by_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have now completed the two steps but the results really don't stand as clear enough. We need to sort through this list and print the five highest values in a format that is readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.137546468401487, '02'], [7.407801418439717, '01'], [8.804177545691905, '22'], [8.687258687258687, '21'], [7.163043478260869, '19'], [9.449744463373083, '17'], [28.676470588235293, '15'], [9.692007797270955, '14'], [16.31756756756757, '13'], [8.96474358974359, '11'], [10.684397163120567, '10'], [6.653153153153153, '09'], [7.013274336283186, '07'], [7.948339483394834, '03'], [6.696793002915452, '23'], [8.749019607843136, '20'], [7.713298791018998, '16'], [9.190661478599221, '08'], [7.5647840531561465, '00'], [7.94299674267101, '18'], [12.380116959064328, '12'], [9.7119341563786, '04'], [6.782051282051282, '06'], [8.794258373205741, '05']]\n",
      "\n",
      "\n",
      "Top 5 Hours for Ask Post Comments\n",
      "15:00: 28.68 average comments per post\n",
      "13:00: 16.32 average comments per post\n",
      "12:00: 12.38 average comments per post\n",
      "02:00: 11.14 average comments per post\n",
      "10:00: 10.68 average comments per post\n"
     ]
    }
   ],
   "source": [
    "swap_avg_by_hour = []\n",
    "\n",
    "for row in avg_by_hour:\n",
    "    swap_avg_by_hour.append([row[1], row[0]])\n",
    "    \n",
    "print (swap_avg_by_hour)\n",
    "print ('\\n')\n",
    "\n",
    "sorted_swap = sorted(swap_avg_by_hour, reverse = True)\n",
    "\n",
    "print (\"Top 5 Hours for Ask Post Comments\")\n",
    "for average, hour in sorted_swap[:5]:\n",
    "    print(\n",
    "        \"{}: {:.2f} average comments per post\".format(\n",
    "            dt.datetime.strptime(hour, \"%H\").strftime(\"%H:%M\"),average\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CONCLUSION**\n",
    "\n",
    "- From the above analysis we can now deduce that:\n",
    "\n",
    "    - The best hour of the day to make ask post comments is at 15:00 which has an average of 28.68 comments per post.\n",
    "\n",
    "\n",
    "- In this lesson, we have learnt how to analyse through social media posts with focus on **Hacker News** and we conducted an analysis on which of the posts on the platforom  gains the maximum average amount of comments. We also further analysed the posts with the highest average based on the `timedate` class which we learnt earlier. This allowed us to be able to determine which hour of the day attracts the highest number of comments. These results will help existing and prospective users of the platform to have a good idea of when to make Ask Posts in order to get a subatantial number of contributions.\n",
    "\n",
    "\n",
    "- Think you can do this on your own? I CHALLENGE YOU! GO FOR IT!!!!\n",
    "\n",
    "\n",
    "- And remember, YOU CAN DO IT!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
